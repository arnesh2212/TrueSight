{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: torch in /home/ayush/.local/lib/python3.12/site-packages (2.4.0+cu124)\n",
      "Collecting streamlit\n",
      "  Using cached streamlit-1.39.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: pandas in /home/ayush/.local/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /home/ayush/.local/lib/python3.12/site-packages (1.26.3)\n",
      "Collecting pymupdf\n",
      "  Using cached PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting tf-keras\n",
      "  Using cached tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting llama-cpp-python\n",
      "  Using cached llama_cpp_python-0.3.1.tar.gz (63.9 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ultralytics\n",
      "  Using cached ultralytics-8.3.27-py3-none-any.whl.metadata (35 kB)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: torchvision in /home/ayush/.local/lib/python3.12/site-packages (0.19.0+cu124)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3.12/site-packages (from transformers) (3.13.3)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ayush/.local/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ayush/.local/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Using cached tokenizers-0.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.6-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ayush/.local/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/ayush/.local/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ayush/.local/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ayush/.local/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ayush/.local/lib/python3.12/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /home/ayush/.local/lib/python3.12/site-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /home/ayush/.local/lib/python3.12/site-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /home/ayush/.local/lib/python3.12/site-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ayush/.local/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /home/ayush/.local/lib/python3.12/site-packages (from torch) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /home/ayush/.local/lib/python3.12/site-packages (from torch) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /home/ayush/.local/lib/python3.12/site-packages (from torch) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /home/ayush/.local/lib/python3.12/site-packages (from torch) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /home/ayush/.local/lib/python3.12/site-packages (from torch) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ayush/.local/lib/python3.12/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /home/ayush/.local/lib/python3.12/site-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /home/ayush/.local/lib/python3.12/site-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/ayush/.local/lib/python3.12/site-packages (from torch) (3.0.0)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Using cached blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/lib/python3.12/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /home/ayush/.local/lib/python3.12/site-packages (from streamlit) (10.2.0)\n",
      "Collecting protobuf<6,>=3.20 (from streamlit)\n",
      "  Using cached protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Using cached pyarrow-18.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /home/ayush/.local/lib/python3.12/site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: watchdog<6,>=2.1.5 in /home/ayush/.local/lib/python3.12/site-packages (from streamlit) (5.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ayush/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ayush/.local/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ayush/.local/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Collecting tensorflow<2.19,>=2.18 (from tf-keras)\n",
      "  Using cached tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /home/ayush/.local/lib/python3.12/site-packages (from ultralytics) (3.9.2)\n",
      "Collecting scipy>=1.4.1 (from ultralytics)\n",
      "  Using cached scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: psutil in /home/ayush/.local/lib/python3.12/site-packages (from ultralytics) (6.0.0)\n",
      "Collecting py-cpuinfo (from ultralytics)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting seaborn>=0.11.0 (from ultralytics)\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Using cached ultralytics_thop-2.0.10-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting narwhals>=1.5.2 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached narwhals-1.12.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ayush/.local/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ayush/.local/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ayush/.local/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ayush/.local/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ayush/.local/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/ayush/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.12/site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3.12/site-packages (from requests->transformers) (1.26.20)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ayush/.local/lib/python3.12/site-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.4.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Using cached wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Using cached grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Using cached keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Using cached h5py-3.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Using cached ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: platformdirs in /home/ayush/.local/lib/python3.12/site-packages (from setuptools->torch) (4.3.6)\n",
      "Requirement already satisfied: jaraco.text in /usr/lib/python3.12/site-packages (from setuptools->torch) (4.0.0)\n",
      "Requirement already satisfied: more-itertools in /usr/lib/python3.12/site-packages (from setuptools->torch) (10.3.0)\n",
      "Requirement already satisfied: ordered-set in /usr/lib/python3.12/site-packages (from setuptools->torch) (4.1.0)\n",
      "Requirement already satisfied: tomli in /usr/lib/python3.12/site-packages (from setuptools->torch) (2.0.1)\n",
      "Requirement already satisfied: validate-pyproject in /usr/lib/python3.12/site-packages (from setuptools->torch) (0.21)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ayush/.local/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.44.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.1.dev0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached rpds_py-0.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Using cached optree-0.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Using cached werkzeug-3.1.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: jaraco.functools in /usr/lib/python3.12/site-packages (from jaraco.text->setuptools->torch) (4.0.2)\n",
      "Requirement already satisfied: jaraco.context>=4.1 in /usr/lib/python3.12/site-packages (from jaraco.text->setuptools->torch) (5.3.0)\n",
      "Requirement already satisfied: autocommand in /usr/lib/python3.12/site-packages (from jaraco.text->setuptools->torch) (2.2.2)\n",
      "Requirement already satisfied: fastjsonschema<=3,>=2.16.2 in /usr/lib/python3.12/site-packages (from validate-pyproject->setuptools->torch) (2.20.0)\n",
      "Using cached transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
      "Using cached streamlit-1.39.0-py2.py3-none-any.whl (8.7 MB)\n",
      "Using cached PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.8 MB)\n",
      "Using cached tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached ultralytics-8.3.27-py3-none-any.whl (878 kB)\n",
      "Using cached opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "Using cached altair-5.4.1-py3-none-any.whl (658 kB)\n",
      "Using cached blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Using cached huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Using cached protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Using cached pyarrow-18.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "Using cached scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.5/615.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:13\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading tqdm-4.66.6-py3-none-any.whl (78 kB)\n",
      "Downloading ultralytics_thop-2.0.10-py3-none-any.whl (26 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Downloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-1.12.1-py3-none-any.whl (195 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:02\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (87 kB)\n",
      "Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (364 kB)\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.1-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (362 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.1-cp312-cp312-linux_x86_64.whl size=3534102 sha256=ffd3226a5e97bfef3767eb73ad4cb74d6bf642ed9e2f23287e3c85e9f1b30f34\n",
      "  Stored in directory: /home/ayush/.cache/pip/wheels/58/d4/b6/f219a2c6af82353b2a21923250728c3d180c95a5ad9ec1c6c3\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: py-cpuinfo, namex, libclang, flatbuffers, wrapt, werkzeug, tqdm, toml, tensorboard-data-server, tenacity, smmap, scipy, safetensors, rpds-py, pymupdf, pyarrow, protobuf, optree, opt-einsum, opencv-python, narwhals, ml-dtypes, mdurl, h5py, grpcio, google-pasta, gast, diskcache, cachetools, blinker, astunparse, absl-py, referencing, pydeck, markdown-it-py, llama-cpp-python, huggingface-hub, gitdb, tokenizers, seaborn, rich, jsonschema-specifications, gitpython, transformers, tensorboard, keras, jsonschema, ultralytics-thop, tensorflow, altair, ultralytics, tf-keras, streamlit\n",
      "Successfully installed absl-py-2.1.0 altair-5.4.1 astunparse-1.6.3 blinker-1.8.2 cachetools-5.5.0 diskcache-5.6.3 flatbuffers-24.3.25 gast-0.6.0 gitdb-4.0.11 gitpython-3.1.43 google-pasta-0.2.0 grpcio-1.67.1 h5py-3.12.1 huggingface-hub-0.26.2 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 keras-3.6.0 libclang-18.1.1 llama-cpp-python-0.3.1 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 narwhals-1.12.1 opencv-python-4.10.0.84 opt-einsum-3.4.0 optree-0.13.0 protobuf-5.28.3 py-cpuinfo-9.0.0 pyarrow-18.0.0 pydeck-0.9.1 pymupdf-1.24.13 referencing-0.35.1 rich-13.9.4 rpds-py-0.20.1 safetensors-0.4.5 scipy-1.14.1 seaborn-0.13.2 smmap-5.0.1 streamlit-1.39.0 tenacity-9.0.0 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tf-keras-2.18.0 tokenizers-0.20.1 toml-0.10.2 tqdm-4.66.6 transformers-4.46.1 ultralytics-8.3.27 ultralytics-thop-2.0.10 werkzeug-3.1.1 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch streamlit pandas numpy pymupdf tf-keras llama-cpp-python ultralytics opencv-python torchvision --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from io import BytesIO\n",
    "import os\n",
    "import argparse\n",
    "import re\n",
    "import fitz\n",
    "\n",
    "def search_for_text(lines, search_str):\n",
    "    \"\"\"\n",
    "    Search for the search string within the document lines\n",
    "    \"\"\"\n",
    "    for line in lines:\n",
    "        # Find all matches within one line\n",
    "        results = re.findall(search_str, line, re.IGNORECASE)\n",
    "        # In case multiple matches within one line\n",
    "        for result in results:\n",
    "            yield result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 'sample' on page 1\n",
      "Page text: 1.\n",
      "Text File\n",
      "a.\n",
      "PII identification (https://github.com/i-dot-ai/redactomatic)\n",
      "b.\n",
      "Advanced - Replace with scenthetic information / scramble text\n",
      "c.\n",
      "Ensure no input data is stored or retrievable after processing.\n",
      "d.\n",
      "https://www.youtube.com/watch?v=nJKNLl9W-2E\n",
      "2.\n",
      "Images / VIDEOS\n",
      "a.\n",
      "OCR for Text: Extract text from images and apply the same redaction as the text\n",
      "modality.\n",
      "b.\n",
      "For documents or images containing sensitive objects (e.g., license plates,\n",
      "sensitive labels), detect and redact these areas.\n",
      "c.\n",
      "Grounding Dino for custom\n",
      "d.\n",
      "BLURING / BLACK BOX / PIXELATION\n",
      "e.\n",
      "(replacement with synthetic objects)\n",
      "3.\n",
      "PDF\n",
      "a.\n",
      "https://stackoverflow.com/questions/75387339/how-can-i-edit-modify-replace-text\n",
      "-in-an-existing-pdf-file\n",
      "b.\n",
      "Same as text\n",
      "c.\n",
      "Resources -\n",
      "https://huggingface.co/docs/transformers/tasks/token_classification\n",
      "https://huggingface.co/datasets/ai4privacy/pii-masking-300k\n",
      "https://medium.com/@bisinet/obfuscating-python-code-f6537e5c51a3\n",
      "https://microsoft.github.io/presidio/samples/python/example_dicom_image_redactor/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"demo.pdf\"\n",
    "search_str = \"sample\"\n",
    "\n",
    "# Open the PDF file\n",
    "pdf_document = fitz.open(file_path)\n",
    "\n",
    "# Iterate over all pages\n",
    "for page_num in range(pdf_document.page_count):\n",
    "    # Get the page\n",
    "    page = pdf_document.load_page(page_num)\n",
    "    # Get the text\n",
    "    text = page.get_text()\n",
    "    # Split the text into lines\n",
    "    lines = text.split(\"\\n\")\n",
    "    # Search for the search string within the lines\n",
    "    for result in search_for_text(lines, search_str):\n",
    "        print(f\"Found '{result}' on page {page_num}\")\n",
    "        print(f\"Page text: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "\n",
    "# Open the PDF document\n",
    "doc = pymupdf.open('demo.pdf')\n",
    "\n",
    "# Iterate over each page of the document\n",
    "for page in doc:\n",
    "    # Find all instances of \"Jane Doe\" on the current page\n",
    "    instances = page.search_for(\"sample\")\n",
    "\n",
    "    # Redact each instance of \"Jane Doe\" on the current page\n",
    "    for inst in instances:\n",
    "        page.add_redact_annot(inst , fill=(0,0,0))\n",
    "\n",
    "    # Apply the redactions to the current page\n",
    "    page.apply_redactions(graphics=0)\n",
    "\n",
    "# Save the modified document\n",
    "doc.save('output.pdf')\n",
    "\n",
    "# Close the document\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the text\n",
    "doc = pymupdf.open('demo.pdf')\n",
    "\n",
    "text = \"\"\n",
    "for page in doc:\n",
    "    text += page.get_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Background: Easy to use and secure redaction tool “RE-DACT” which allows\\nredaction/masking/anonymization on various input formats based on a gradational scale defined\\nby the user and providing customized output. Over a time, model will learn and have the ability to\\ngenerate realistic synthetic data in any sought format. Description: The proposed solution is a\\nnatural language processing (machine learning) based redaction tool. The tool will redact or\\nobfuscate from original data leaving the output structurally/logically the same but stripped of key\\nidentifiers and other content which may in any way allow the identity, actual data, markers or\\nissues in the input content to be revealed. The correlational logic may be appropriately\\nobfuscated based on the degree of redaction. This will have an easy to use GUI and will be\\navailable for use on online and offline systems. The degree of the redaction will be up to the user-\\nthe higher the degree set by the user, the more the degree of redaction. This will work with all\\ndifferent commonly used formats for text and data sets. Security of data will be assured by\\nensuring that the input data is not stored or retrievable in any fashion by third party entities. User\\nwill have complete control over the input data. It is also an important aspect that sometimes data\\nmay be required to be stored or submitted, however specific sensitive details may not necessarily\\nbe required. In such a situation- anonymized data authenticated as having being redacted from\\noriginal would suffice. Declassification processes are long and arduous; anonymization is largely\\nmanual or custom script driven. By providing a gradational redaction option, ordinary users can\\nstrip away the specificity to the extent of liking-from merely name removal/anonymization to\\ncompletely synthetic data with only faint traces of original structure/pattern. This can allow\\ngeneration of large number of databases with realistic but anonymized data that can be shared\\nfor learning, growth and commercial ventures. Expected Solution: Problem Statement: Easy to\\nuse and secure redaction tool “RE-DACT” which allows redaction/masking/anonymization on\\nvarious input formats based on a gradational scale defined by the user and providing customized\\noutput. Over a time, model will learn and have the ability to generate realistic synthetic data in\\nany sought format. Stage 1 Data: Curate and use own data set for building PoC Task/Result:\\nInput/Output: Supports common input formats (text files, images) and basic output formats\\n(redacted files, logs). Web based version Training Dataset: Publicly available dataset can be used\\nfor the purpose. Metrics: Precision, Recall, F1 Score on Open Source Testing dataset. Stage 2\\nData: Dataset will be provided in Grand Finale Task/Result: Input/Output: Expands to handle\\nmore data formats (e.g., PDFs, videos) and advanced output options (e.g., redacted versions with\\nannotations). Training Dataset: Diverse data set prepared from commonly used formats Metrics:\\nPrecision, Recall, F1 Score on Open Source Testing dataset. Performance/Evaluation Criteria: •\\nPoC will be preferred over just concept or presentation. The performance may be ascertained on\\nthe following metrics: • Efficacy of the redaction/anonymization- whether appropriate data has\\nbeen redacted • Gradational effect achieved based on user preference and ability to calibrate. •\\nAbility to work on a variety of input sources • Security of the input data by minimal retention •\\nSpeed • Optimized computing usage and ability to operate at scale. • Ease of use, UI, UX. •\\nPerformance benchmarked against COTS solutions. • Web Based and Offline solution. • Minimal\\nAPI dependency • Use of Secure Coding Practices and cybersecurity built in design\\n1.\\nText File\\na.\\nPII identification (https://github.com/i-dot-ai/redactomatic)\\nb.\\nAdvanced - Replace with scenthetic information / scramble text\\nc.\\nEnsure no input data is stored or retrievable after processing.\\nd.\\nhttps://www.youtube.com/watch?v=nJKNLl9W-2E\\n2.\\nImages / VIDEOS\\na.\\nOCR for Text: Extract text from images and apply the same redaction as the text\\nmodality.\\nb.\\nFor documents or images containing sensitive objects (e.g., license plates,\\nsensitive labels), detect and redact these areas.\\nc.\\nGrounding Dino for custom\\nd.\\nBLURING / BLACK BOX / PIXELATION\\ne.\\n(replacement with synthetic objects)\\n3.\\nPDF\\na.\\nhttps://stackoverflow.com/questions/75387339/how-can-i-edit-modify-replace-text\\n-in-an-existing-pdf-file\\nb.\\nSame as text\\nc.\\nResources -\\nhttps://huggingface.co/docs/transformers/tasks/token_classification\\nhttps://huggingface.co/datasets/ai4privacy/pii-masking-300k\\nhttps://medium.com/@bisinet/obfuscating-python-code-f6537e5c51a3\\nhttps://microsoft.github.io/presidio/samples/python/example_dicom_image_redactor/\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 22:08:39.377155: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-27 22:08:39.461922: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-27 22:08:39.476517: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-27 22:08:39.574645: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-27 22:08:40.689626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"token-classification\", model=\"output_mdeberta_base/checkpoint-320000\" ,  aggregation_strategy=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnesh/.local/lib/python3.12/site-packages/transformers/pipelines/token_classification.py:392: UserWarning: Tokenizer does not support real words, using fallback heuristic\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'ZIPCODE',\n",
       "  'score': 0.9792691,\n",
       "  'word': '1234567891',\n",
       "  'start': 0,\n",
       "  'end': 10}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"1234567891\"\n",
    "pipe(text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = pipe.model.config.id2label\n",
    "label2id = pipe.model.config.label2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-CITY', 'I-CITY', 'B-FIRSTNAME', 'I-FIRSTNAME', 'B-ZIPCODE', 'I-ZIPCODE', 'B-DATE', 'I-DATE', 'B-CREDITCARDISSUER', 'I-CREDITCARDISSUER', 'B-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'B-CREDITCARDCVV', 'B-USERNAME', 'I-USERNAME', 'B-JOBTYPE', 'B-PREFIX', 'I-PREFIX', 'B-LASTNAME', 'B-EMAIL', 'I-EMAIL', 'B-IP', 'I-IP', 'B-PHONEIMEI', 'I-PHONEIMEI', 'B-MAC', 'I-MAC', 'B-DOB', 'I-DOB', 'B-NEARBYGPSCOORDINATE', 'I-NEARBYGPSCOORDINATE', 'B-BUILDINGNUMBER', 'I-BUILDINGNUMBER', 'B-IPV4', 'I-IPV4', 'B-BITCOINADDRESS', 'I-BITCOINADDRESS', 'B-SSN', 'I-SSN', 'B-PHONENUMBER', 'I-PHONENUMBER', 'B-SEX', 'B-SECONDARYADDRESS', 'I-SECONDARYADDRESS', 'B-ACCOUNTNUMBER', 'I-ACCOUNTNUMBER', 'B-ACCOUNTNAME', 'I-ACCOUNTNAME', 'B-STREET', 'I-STREET', 'B-MASKEDNUMBER', 'I-MASKEDNUMBER', 'B-COMPANYNAME', 'I-COMPANYNAME', 'B-CURRENCYSYMBOL', 'B-IBAN', 'I-IBAN', 'I-AMOUNT', 'B-IPV6', 'I-IPV6', 'B-MIDDLENAME', 'I-MIDDLENAME', 'B-COUNTY', 'I-COUNTY', 'B-AGE', 'B-PASSWORD', 'I-PASSWORD', 'B-STATE', 'I-STATE', 'B-URL', 'I-URL', 'I-JOBTYPE', 'B-USERAGENT', 'I-USERAGENT', 'I-AGE', 'B-ORDINALDIRECTION', 'B-JOBTITLE', 'I-JOBTITLE', 'B-ETHEREUMADDRESS', 'I-ETHEREUMADDRESS', 'B-VEHICLEVRM', 'I-VEHICLEVRM', 'B-VEHICLEVIN', 'I-VEHICLEVIN', 'B-JOBAREA', 'B-PIN', 'I-PIN', 'I-LASTNAME', 'B-AMOUNT', 'B-CURRENCYCODE', 'I-CURRENCYCODE', 'B-TIME', 'I-TIME', 'B-HEIGHT', 'I-HEIGHT', 'B-EYECOLOR', 'B-GENDER', 'I-GENDER', 'B-BIC', 'I-BIC', 'B-CURRENCY', 'I-CURRENCY', 'I-CURRENCYSYMBOL', 'B-CURRENCYNAME', 'I-CURRENCYNAME', 'B-LITECOINADDRESS', 'I-LITECOINADDRESS', 'I-JOBAREA', 'I-EYECOLOR', 'I-CREDITCARDCVV', 'I-SEX']\n"
     ]
    }
   ],
   "source": [
    "labels = list(id2label.values())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dict = {\n",
    "    \"NAME\": ['B-FIRSTNAME', 'I-FIRSTNAME', 'B-LASTNAME', 'I-LASTNAME', 'B-MIDDLENAME', 'I-MIDDLENAME'],\n",
    "    \"CITY\": ['B-CITY', 'I-CITY'],\n",
    "    \"ZIPCODE\": ['B-ZIPCODE', 'I-ZIPCODE'],\n",
    "    \"DATE\": ['B-DATE', 'I-DATE'],\n",
    "    \"CREDITCARD\": ['B-CREDITCARDISSUER', 'I-CREDITCARDISSUER', 'B-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'B-CREDITCARDCVV', 'I-CREDITCARDCVV'],\n",
    "    \"USERNAME\": ['B-USERNAME', 'I-USERNAME'],\n",
    "    \"JOB\": ['B-JOBTYPE', 'I-JOBTYPE', 'B-JOBTITLE', 'I-JOBTITLE', 'B-JOBAREA', 'I-JOBAREA'],\n",
    "    \"PREFIX\": ['B-PREFIX', 'I-PREFIX'],\n",
    "    \"EMAIL\": ['B-EMAIL', 'I-EMAIL'],\n",
    "    \"IP\": ['B-IP', 'I-IP', 'B-IPV4', 'I-IPV4', 'B-IPV6', 'I-IPV6'],\n",
    "    \"PHONE\": ['B-PHONENUMBER', 'I-PHONENUMBER', 'B-PHONEIMEI', 'I-PHONEIMEI'],\n",
    "    \"MAC\": ['B-MAC', 'I-MAC'],\n",
    "    \"DOB\": ['B-DOB', 'I-DOB'],\n",
    "    \"GPS\": ['B-NEARBYGPSCOORDINATE', 'I-NEARBYGPSCOORDINATE'],\n",
    "    \"ADDRESS\": ['B-BUILDINGNUMBER', 'I-BUILDINGNUMBER', 'B-STREET', 'I-STREET', 'B-SECONDARYADDRESS', 'I-SECONDARYADDRESS'],\n",
    "    \"BITCOIN\": ['B-BITCOINADDRESS', 'I-BITCOINADDRESS'],\n",
    "    \"SSN\": ['B-SSN', 'I-SSN'],\n",
    "    \"SEX\": ['B-SEX', 'I-SEX', 'B-GENDER', 'I-GENDER'],\n",
    "    \"ACCOUNT\": ['B-ACCOUNTNUMBER', 'I-ACCOUNTNUMBER', 'B-ACCOUNTNAME', 'I-ACCOUNTNAME'],\n",
    "    \"CURRENCY\": ['B-CURRENCYSYMBOL', 'I-CURRENCYSYMBOL', 'B-CURRENCY', 'I-CURRENCY', 'B-AMOUNT', 'I-AMOUNT', 'B-CURRENCYCODE', 'I-CURRENCYCODE', 'B-CURRENCYNAME', 'I-CURRENCYNAME'],\n",
    "    \"IBAN\": ['B-IBAN', 'I-IBAN'],\n",
    "    \"BIC\": ['B-BIC', 'I-BIC'],\n",
    "    \"URL\": ['B-URL', 'I-URL'],\n",
    "    \"PASSWORD\": ['B-PASSWORD', 'I-PASSWORD'],\n",
    "    \"STATE\": ['B-STATE', 'I-STATE'],\n",
    "    \"COUNTY\": ['B-COUNTY', 'I-COUNTY'],\n",
    "    \"AGE\": ['B-AGE', 'I-AGE'],\n",
    "    \"TIME\": ['B-TIME', 'I-TIME'],\n",
    "    \"HEIGHT\": ['B-HEIGHT', 'I-HEIGHT'],\n",
    "    \"EYECOLOR\": ['B-EYECOLOR', 'I-EYECOLOR'],\n",
    "    \"COMPANYNAME\": ['B-COMPANYNAME', 'I-COMPANYNAME'],\n",
    "    \"ETHEREUM\": ['B-ETHEREUMADDRESS', 'I-ETHEREUMADDRESS'],\n",
    "    \"LITECOIN\": ['B-LITECOINADDRESS', 'I-LITECOINADDRESS'],\n",
    "    \"VEHICLE\": ['B-VEHICLEVRM', 'I-VEHICLEVRM', 'B-VEHICLEVIN', 'I-VEHICLEVIN'],\n",
    "    \"ORDINALDIRECTION\": ['B-ORDINALDIRECTION'],\n",
    "    \"USERAGENT\": ['B-USERAGENT', 'I-USERAGENT'],\n",
    "    \"PIN\": ['B-PIN', 'I-PIN'],\n",
    "    \"MASKEDNUMBER\": ['B-MASKEDNUMBER', 'I-MASKEDNUMBER'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'FIRSTNAME', 'score': 0.99994016, 'word': 'Arnesh', 'start': 0, 'end': 6}, {'entity_group': 'LASTNAME', 'score': 0.9999752, 'word': 'Batra', 'start': 6, 'end': 12}]\n",
      "XXXXX XXXXX\n"
     ]
    }
   ],
   "source": [
    "#Mask in output\n",
    "to_show_entities = [\"NAME\"]\n",
    "to_show_entities_actual = []\n",
    "for entity in to_show_entities:\n",
    "    to_show_entities_actual += entity_dict[entity]\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "text = \"Arnesh Batra\"\n",
    "out = pipe(text)\n",
    "print(out)  \n",
    "for ent_gr  in out:\n",
    "    entity_gr = \"I-\" + ent_gr['entity_group']\n",
    "\n",
    "    if entity_gr in to_show_entities_actual:\n",
    "        text = text.replace(ent_gr['word'], \"XXXXX\")\n",
    "\n",
    "print(text)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
